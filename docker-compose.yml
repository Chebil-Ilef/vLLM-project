services:
  neo4j:
    image: neo4j:5.11
    container_name: bia_neo4j
    environment:
      - NEO4J_AUTH=neo4j/test
    volumes:
      - neo4j_data:/data
    ports:
      - '7474:7474'
      - '7687:7687'

  backend:
    build:
      context: ./chatbot-system
      args:
        USE_VLLM: ${USE_VLLM:-0}
    container_name: bia_backend
    env_file:
      - ./chatbot-system/.env
    environment:
      - FLASK_ENV=production
    ports:
      - '5000:5000'
    depends_on:
      - neo4j
      - vllm
    volumes:
      - ./chatbot-system:/app

  vllm:
    build:
        context: .
        dockerfile: ./chatbot-system/Dockerfile.vllm
    image: vllm:latest
    restart: unless-stopped
    environment:
      MODEL_NAME: "${VLLM_MODEL:-facebook/opt-125m}"
      VLLM_LOGGING_LEVEL: DEBUG
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model "${VLLM_MODEL:-facebook/opt-125m}"
      --host 0.0.0.0
      --port 8000
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    # For docker compose v2 you can instead use:
    gpus: all


  frontend:
    build: ./frontend
    container_name: bia_frontend
    ports:
      - '8080:8080'
    depends_on:
      - backend

volumes:
  neo4j_data:
